\documentclass[twoside,12pt]{article}

\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 }

\usepackage[spanish,activeacute,es-noindentfirst]{babel} %paqueteria para idioma
\spanishdecimal{.}
\usepackage{pstricks}
\usepackage[utf8]{inputenc}
%\usepackage{pstricks-add}


%Para Figuras en Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}

\usepackage{amssymb,amsfonts,amsbsy,amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{float}
% \usepackage{enumitem}
\usepackage{enumerate} %Para poner \begin{enumerate}[a)]
\usepackage[hidelinks]{hyperref}

\usepackage{subfig}

\usepackage{rotating} %para rotar texto en tablas con \begin{sideways}
%\end{sideways}
\usepackage{makecell} %Para dividir celdas de tablas en varias l\'ineas


\theoremstyle{definition}
\newtheorem{probn}{Problema}
\newtheorem{soln}{Solución del problema}

\title{\textsc{Tarea 1\\ {\Large Ciencia de Datos}}}
\author{\large{\textbf{Brain de Jesús Salazar, César Ávila, Iván García}}}
\date{12 de septiembre de 2025}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\footrulewidth}{0.1pt}
\fancyhead[LE]{\footnotesize{\textsc{Tarea 1}}}
\fancyhead[RO]{\footnotesize{\textsc{Ciencia de Datos}}}
\rfoot{\textsc{\thepage}}
\lfoot{\footnotesize{\textsc{Brain de Jesús Salazar, César Ávila, Iván García}}}

\usepackage{listings}

\def\sin{\mathop{\mathrm{\normalfont sen}}\nolimits}
\def\cos{\mathop{\mathrm{\normalfont cos}}\nolimits}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\XX}{\boldsymbol{X}}
\newcommand{\YY}{\boldsymbol{Y}}
\newcommand{\ZZ}{\boldsymbol{Z}}
\newcommand{\variance}{\mathrm{\normalfont Var}}
\newcommand{\covariance}{\mathrm{\normalfont Cov}}
\newcommand{\probability}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\expectation}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\given}{\,\middle|\,}

\begin{document}
\maketitle

% Problema 1

\begin{soln}
Recordemos la definición de la matriz Hat:
\[
H\coloneqq X{({X^T} X)}^{-1}X^T.
\]
Para ver que $H$ es idempotente, notemos que
\[
\begin{split}
  HH &= \left(X{(X^T X)}^{-1}X^T\right)\left(X{(X^T X)}^{-1}X^T\right)\\
  &= X{(X^T X)}^{-1} \left((X^T X) {(X^T X)}^{-1}\right) X^T\\
  &= X{(X^T X)}^{-1} X^T\\
  &= H.
\end{split}
\]
Por consiguiente, $H^2=H$, así que $H$ es idempotente. Por otra parte, veamos que
\[
\begin{split}
  H^T &={\left(X{(X^T X)}^{-1} X^T \right)}^T\\
  & ={{(X^T)}^T \left({(X^T X)}^{-1}\right)}^T {X}^T\\
  & =X {\left({\left(X^T X\right)}^T\right)}^{-1} X^T\\
  & =X {(X^T X)}^{-1} X^T\\
  & =H.
\end{split}
\]
Por lo tanto, $H^T=H$, así que $H$ es simétrica. Como también es idempotente, entonces $H$ es una matriz de proyección. De hecho, es la proyección ortogonal sobre el espacio de columnas de $X$ (que se asume que tiene rango de columnas completo), y se cumple que $\hat{\boldsymbol{Y}}=H\boldsymbol{Y}$, con la interpretación usual de $\hat{Y}$.

Además, el vector de residuales es $\boldsymbol{e}=(I-H)\boldsymbol{Y}$, y como $H(I-H)=0$ por la idempotencia, entonces $H\boldsymbol{Y}$ es la proyección de $Y$ sobre el espacio de columnas de $X$. Esto implica que $h_{ii}$ mide la influencia de la observación $i$-ésima para el ajuste lineal, que es precisamente el \textit{leverage}.
\end{soln}


\newpage
% Problema 2
\begin{soln}
Considere el modelo de regresión lineal $\boldsymbol{Y}=X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ donde $\boldsymbol{\varepsilon}\sim N_n(\boldsymbol{0},\sigma^2 I_n)$, y $X$ es una matriz de dimensiones $n\times p$, que asumimos de rango de columnas completo (de modo que $X^T X$ es invertible). Ahora bien, notemos que $X^T X$ es una matriz de dimensiones $p\times p$, y $X^T$ es de dimensiones $p\times n$. Puesto que 
\[
H=X{(X^T X)}^{-1}X^T,
\]
entonces $H$ es una matriz de $n\times n$, y por la propiedad cíclica de la traza se tiene que
\[
\sum_{i=1}^{n} h_{ii} = tr(H) = tr\left(X{(X^T X)}^{-1}X^T\right) = tr\left(X^T X {(X^T X)}^{-1}\right) = tr(I_p)=p,
\]
en donde $I_p$ es la matriz identidad de dimensiones $p\times p$.

Observemos además que los valores ajustados están dados por $\hat{\boldsymbol{Y}}=HY$, así que, como $\variance[Y]=\sigma^2 I$,
\[
\variance[\hat{\boldsymbol{Y}}] = \sigma^2 H H^T = \sigma^2 H^2 = \sigma^2 H,
\]
en donde hemos usado el ejercicio anterior para ver que $H$ es simétrica e idempotente. Así pues, la ``varianza total'' de la estimación es
\[
\sum_{i=1}^{n} \variance[\hat{\boldsymbol{Y}}_i] = \sigma^2 tr(H) = \sigma^2 p;
\]
es decir, es igual al número de parámetros multiplicado por $\sigma^2$ (la varianza común de los errores). Además, cada elemento $h_{ii}$ de la diagonal de $H$ mide la influencia de la observación $i$-ésima, así que el resultado anterior puede ser interpretado como que el número efectivo de parámetros es justamente $p$.

Por otra parte, mientras mayor es el número de parámetros utilizados, el modelo puede explicar a los datos de una mejor manera, disminuyendo el sesgo, pero esto incrementa la varianza total, así que puede llevar a un sobreajuste, disminuyendo la generalidad del modelo.
    % Usando la descomposición espectral, se puede descomponer a $H$ de la forma 
    % $$H=RR^T,$$
    % basta con reescribir a la matriz diagonal como el producto de dos matrices donde cada matriz tiene en su diagonal a la raíz cuadrada de cada elemento. Por otro lado, observe que ya que $H$ es idempotente se tiene que
    % $$
    % H^2=(RR^T)(RR^T)=H=RR^T,
    % $$
    % por lo que $$R^TR=I_p.$$ 
    % Por otro lado, note que por la propiedad cíclica de la traza
    % $$
    % tr(H)=tr(RR^T)=tr(R^TR)=tr(I_p)=p.
    % $$
    % Por lo tanto, $$
    % \sum_{i=1}^n h_{i i}=p.
    % $$
\end{soln}

\newpage
% Problema 3
\begin{soln}
 Considere el modelo de regresión lineal clásico, $\boldsymbol{Y}=X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ donde $\boldsymbol{\varepsilon}\sim N_n(\boldsymbol{0},\sigma^2 I_n)$ y a la matriz de proyección ortogonal,
\[
H=X{(X^T X)}^{-1}X^T.
\]
Ahora bien, el vector de residuos se puede expresar como 
\[
\boldsymbol{e}=(I_n-H)\boldsymbol{Y},
\]
y como $\boldsymbol{Y}$ sigue una distribución normal multivariada con media $X\boldsymbol{\beta}$ y varianza $\sigma^2 I_n$, se tiene que, 
\[
\boldsymbol{e}\sim N_n\left(\mathbf{0}, \sigma^2\left(I_n-H\right)\right).
\]
De lo anterior se sigue que, para cada $i\in \left\lbrace 1, \ldots, n \right\rbrace$,
\[
e_i \sim N(0,\sigma^2 (1-h_{ii})),
\]
y normalizando,
\[
\frac{e_i}{\sigma\sqrt{1-h_{ii}}}\sim N(0,1).
\]
Por otro lado, observemos que si se considera al estimador insesgado de la varianza,
\[
\hat{\sigma}^2=\frac{\mathbf{e}^{\top} \mathbf{e}}{n-p}=\frac{\sigma^2 \mathbf{Y}^{\top}(I_n-H) \mathbf{Y}}{\sigma^2 (n-p)},
\]
como $(I_n-H)$ es una matriz idempotente de rango $n-p$ (la idempotencia se sigue del ejercicio $1$, y en una matriz idempotente el rango es igual a la traza), se tiene que 
\[
\frac{(n-p)}{\sigma^2}\hat{\sigma}^2= \frac{ \mathbf{Y}^{\top}(I-H) \mathbf{Y}}{\sigma^2} \sim \chi^2(n-p).
\]
Por último, se sabe que la razón entre una variable aleatoria normal estándar sobre la raíz cuadrada de una variable aleatoria independiente con distribución ji cuadrada dividida entre sus $r$ grados, se distribuye como una variable aleatoria t de Student con $r$ grados de libertad. Ya que en este caso los residuos normalizados (con una distribución  normal estándar) no son independientes de la ji cuadrada se tiene que la siguiente variable aleatoria cumple que aproximadamente,
\[
\frac{\frac{e_i}{\sigma\sqrt{1-h_{ii}}}}{\sqrt{\frac{(n-p)}{(n-p)\sigma^2}\hat{\sigma}^2}} \sim t(n-p).
\]
Simplificando la expresión de la izquierda se tiene que los residuos estandarizados tienen una distribución aproximada de:
\[
r_i=\frac{e_i}{\hat{\sigma} \sqrt{1-h_{i i}}} \sim t(n-p).
\]
Para arreglar el problema de la independencia y obtener la distribución que pide el problema, se puede estimar a $\sigma^2$ sin usar el i-ésimo dato. Es decir, definamos
\[
\hat{\sigma}_{i}^2=\frac{\mathbf{e_i}^{\top} \mathbf{e_i}}{n-p-1}=\frac{\sigma^2 \mathbf{Y}_i^{\top}(I_{n-1}-H_i) \mathbf{Y_i}}{\sigma^2 (n-p-1)},
\]
donde el vector $\mathbf{Y}_i$ se obtiene eliminando la $i$-ésima entrada de $Y$ y $H_i$ se construye eliminando la $i$-ésima entrada de $X$. En este caso la matriz $(I_{n-1}-H_i)$ es idempotente y de rango $n-p-1$, por lo que
\[
\frac{(n-1-p)}{\sigma^2}\hat{\sigma_i}^2= \frac{\mathbf{Y_i}^{\top}(I_{n-1}-H_i) \mathbf{Y}_i}{\sigma^2} \sim \chi^2(n-1-p).
\]
Como $\hat{\sigma}_i$ no depende de la $i$-ésima observación, y los errores son independientes, entonces también es independiente de $e_i$. De aquí se tiene la siguiente distribución exacta:
\[
\frac{\frac{e_i}{\sigma\sqrt{1-h_{ii}}}}{\sqrt{\frac{(n-1-p)}{(n-1-p)\sigma^2}\hat{\sigma_i}^2}} \sim t(n-1-p).
\]
Simplificando el lado derecho se concluye que, de manera exacta,
\[
\frac{e_i}{\hat{\sigma}_i \sqrt{1-h_{i i}}}\sim t(n-p-1).
\]
Gracias a lo demostrado anteriormente, para cada observación $i$ se puede calcular su residuo estandarizado (o studentizado), y bajo la hipótesis nula de que la observación $i$-ésima es bien consistente con el modelo, dicho residuo debería pertenecer a una distribución $t$ de Student con los grados de libertad anteriormente mencionados, así que al calcular su $p$-valor se puede establecer un criterio que permita la detección de outliers.
\end{soln}

\newpage
% Problema 4
\begin{soln}
Sean $\boldsymbol{Y}=(Y_{obs},Y_{mis})$, $\boldsymbol{R}$ el patrón de datos faltantes, $\theta$ los parámetros del modelo y $\psi$ los parámetros del mecanismo de faltantes. Por la definición de MCAR,
\[
\probability{\boldsymbol{R}\given \psi} =\probability{\boldsymbol{R}\given Y_{obs},Y_{mis}, \theta, \psi}=\probability{\boldsymbol{R}\given \boldsymbol{Y}, \theta, \psi}.
\]
Luego,
\[
\begin{split}
    \probability{\boldsymbol{Y},\boldsymbol{R}\given \theta, \psi} &= 
    \probability{\boldsymbol{R}\given \boldsymbol{Y},\theta, \psi}\probability{\boldsymbol{Y}\given \theta, \psi} \\
    &= \probability{\boldsymbol{R}\given \psi}\probability{\boldsymbol{Y}\given \theta, \psi} \\
    &= \probability{\boldsymbol{R}\given \psi}\probability{\boldsymbol{Y}\given \theta},
\end{split}
\]
pues el mecanismo de faltantes es independiente de los datos, que es lo que queríamos probar. Por lo tanto, la verosimilitud de datos observados para $\theta$ es
\[
\begin{split}
  L_{obs}(\theta) = \int \probability{\boldsymbol{Y}, \boldsymbol{R} \given \theta, \psi} dY_{mis} &= \int \probability{\boldsymbol{R}\given \psi}\probability{Y_{obs}, Y_{mis}\given \theta} dY_{mis}\\
  &= \probability{\boldsymbol{R}\given \psi}\int \probability{Y_{obs}, Y_{mis}\given \theta} dY_{mis}\\
  & \propto \int \probability{Y_{obs}, Y_{mis}\given \theta} dY_{mis}\\
  &= \probability{Y_{obs} \given \theta}.
\end{split}
\]
Por lo tanto, la inferencia sobre $\theta$ puede basarse únicamente en $\probability{Y_{obs} \given \theta}$, ignorando $\probability{\boldsymbol{R}\given \psi}$. 
\end{soln}

\newpage
% Problema 5
\begin{soln}
Consideremos una muestra $Y_1, \ldots, Y_n$ de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza $\sigma^2$, con indicadores $R_i \in \left\lbrace 0, 1 \right\rbrace$, de tal manera que $R_i\perp Y_i$ para cada $i\in \left\lbrace 1, \ldots, n \right\rbrace$. Dichas $R_i$ existen pues estamos bajo el modelo MCAR, y se interpretan como $R_i = 1$ si y solo si el dato $Y_i$ fue observado.

Notemos que si $n_{obs}$ representa el número de datos observados, entonces $n_{obs} = \sum_{i=1}^{n} R_i$. Además, por la definición de los $R_i$,
\[
\overline{Y}_{obs} = \frac{1}{n_{obs}} \sum_{i=1}^{n} R_i Y_i = \frac{1}{n_{obs}} \sum_{i: R_i=1} Y_i.
\]
Así pues, si $\boldsymbol{R}=(R_1, \ldots, R_n)$, entonces
\[
\begin{split}
  \expectation{\overline{Y}_{obs} \given \boldsymbol{R}} = \expectation{\frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i Y_i  \given \boldsymbol{R}} &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} \expectation{R_i Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i}\\
  &=\mu,
\end{split}
\]
en donde hemos usado que las $Y_i$ son iid con media $\mu$ y son independientes de $\boldsymbol{R}$ (por las hipótesis del modelo MCAR). Por consiguiente,
\[
\expectation{\overline{Y}_{obs}} = \expectation{\expectation{\overline{Y}_{obs} \given \boldsymbol{R}}} = \mu.
\]
Por otra parte,
\[
\overline{Y}_{obs}^2 = \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n}R_i^2 Y_i^2 + \sum_{i\neq j}R_i R_j Y_i Y_j\right],
\]
por lo que
\[
\begin{split}
  \expectation{\overline{Y}_{obs}^2 \given \boldsymbol{R}} &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2 \given \boldsymbol{R}} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j \given \boldsymbol{R}}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[(\sigma^2+\mu^2)\sum_{i=1}^{n} R_i^2 + \mu^2\sum_{i\neq j}R_i R_j\right]\\
  &= \mu^2+\sigma^2\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}.
\end{split}
\]
De lo anterior se sigue que
\[
\begin{split}
  \variance \left[\overline{Y}_{obs}\right] = \expectation{\overline{Y}_{obs}^2}-{\left(\expectation{\overline{Y}_{obs}}\right)}^2 &= \expectation{\expectation{\overline{Y}_{obs}^2\given \boldsymbol{R}}}-\mu^2
  = \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{1}{\sum_{i=1}^{n} R_i}}\\
  &= \sigma^2 \expectation{\frac{1}{n_{obs}}}\\
  &\geq \frac{\sigma^2}{n} = \variance \left[\ \overline{Y}\ \right],
\end{split}
\]
en donde hemos usado que $R_i^2 = R_i$, pues $R_i\in \left\lbrace 0, 1 \right\rbrace$, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, y que $n_{obs}\leq n$. De hecho, siguiendo un procedimiento completamente análogo, pero ahora con varianzas condicionales, se sigue que
\[
\variance \left[\overline{Y}_{obs} \given \boldsymbol{R}\right] = \frac{\sigma^2}{n_{obs}}.
\]
De lo anterior podemos notar que $\overline{Y}_{obs}$ es insesgado, pero que $\variance \left[\overline{Y}_{obs}\right] \geq \variance \left[\ \overline{Y}\ \right]$, por lo que $\overline{Y}_{obs}$ tiene menor eficiencia (posee más varianza, pues la eliminación de datos hace que haya menos de ellos para poder estimar a la media de $Y$).
\end{soln}

\newpage
% Problema 6
\begin{soln}
Sean $\YY = (Y_{obs}, Y_{mis})$ y $\boldsymbol{R}$ el patrón de datos faltantes. Bajo la definición del MAR,
\[
\probability{\boldsymbol{R} \given Y_{obs}, Y_{mis}, \theta, \psi} = \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Así pues, bajo este modelo,
\[
\probability{\YY, \boldsymbol{R} \given \theta, \psi} = \probability{\YY \given \theta} \probability{\boldsymbol{R} \given \YY, \psi} =  \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Por consiguiente, la verosimilitud de $\theta$ está dada por
\[
\begin{split}
  L\left(\theta; Y_{obs}, \boldsymbol{R}\right) = \int \probability{\YY, \boldsymbol{R} \given \theta, \psi} dY_{mis} &= \int \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \int \probability{\YY \given \theta} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta}.
\end{split}
\]
Ya que el factor $\probability{\boldsymbol{R} \given Y_{obs}, \psi}$ no depende de $\theta$, se sigue que $L\left(\theta; Y_{obs}, \boldsymbol{R}\right) \propto \probability{Y_{obs} \given \theta}$. Para ver las condiciones \textit{a priori} que garantizan ignorabilidad bajo el enfoque bayesiano, notemos que
\[
\begin{split}
  \probability{\theta \given Y_{obs}, \boldsymbol{R}} = \int \probability{\theta, \psi \given Y_{obs}, \boldsymbol{R}} d\psi &\propto \int \probability{Y_{obs}, \boldsymbol{R} \given \theta, \psi} \pi\left(\theta, \psi\right) d\psi\\
  &= \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta} \pi\left(\theta, \psi\right) d\psi\\
  &= \probability{Y_{obs} \given \theta} \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \pi\left(\theta, \psi\right) d\psi.
\end{split}
\]
Para concluir ignorabilidad buscamos que $L\left(\theta \given Y_{obs}, \boldsymbol{R}\right) \propto \pi(\theta) \probability{Y_{obs} \given \theta}$, y ya que la integral anterior depende de $\theta$ solamente a través del factor $\pi\left(\theta, \psi\right)$, dicha ignorabilidad se logra cuando $\pi(\theta, \psi)=\pi(\theta)\pi(\psi)$. Es decir, si en la \textit{a priori} se pide indistinguibilidad de los parámetros (i.e. que $\theta$ y $\psi$ sean independientes), entonces el mecanismo es ignorable para inferir $\theta$.
\end{soln}

\newpage
% Problema 7
\begin{soln}
Sean $\hat{\boldsymbol{\beta}}$ los coeficientes estimados de la regresión completa, y $\hat{\boldsymbol{\beta}}_{(i)}$ los coeficientes estimados sin la observación $i$-ésima. Es un hecho conocido que para la regresión lineal se cumple que
\begin{equation}\label{Prob7-Eq1}
  \hat{\boldsymbol{\beta}}_{(i)} = \hat{\boldsymbol{\beta}} - {(X^{T} X)}^{-1}\boldsymbol{x}_i \frac{e_i}{1-h_{ii}}.
\end{equation}
Para ver lo anterior, supongamos sin pérdida de generalidad que $i=1$ (el resto de los argumentos son análogos). Podemos particionar la información a eliminar de la siguiente manera:
\[
\boldsymbol{Y}=\left[ \begin{array}{c}
Y_1 \\ \boldsymbol{Y}_{(1)} \end{array} \right] =
X \boldsymbol{\beta} + \boldsymbol{\epsilon} = \left[ \begin{array}{c}
\boldsymbol{x}_1^{T} \\ X_1 \end{array} \right] \boldsymbol{\beta} + \boldsymbol{\epsilon},
\]
en donde $(1)$ significa el vector sin la observación $1$, y $\boldsymbol{x}_1^{T}$ es la fila $1$ de $X$ (la correspondiente a la observación $1$). De este modo, $\hat{\boldsymbol{\beta}}_{(1)}$ es el ajuste del modelo
\[
\boldsymbol{Y}_{(1)}=X_1\beta+\boldsymbol{\epsilon}_{(1)},
\]
por lo que $\hat{\boldsymbol{\beta}}_{(1)}=(X_1^{T}X_1)^{-1}X_1^{T}\boldsymbol{Y}_{(1)}$. Por otro lado, veamos que
\begin{equation}\label{Prob7-Eq2}
  X^{T}X=[\boldsymbol{x}_1,X_1^{T}]\left[ \begin{array}{c} \boldsymbol{x}_1^{T} \\ X_1 \end{array} \right]
= \boldsymbol{x}_1\boldsymbol{x}_1^{T}+X_1^{T}X_1,
\end{equation}
de donde se sigue que $X_1^{T}X_1=X^{T}X-\boldsymbol{x}_1\boldsymbol{x}_1^{T}$. Luego, por la identidad de Woodbury para matrices se tiene que
\[
(X_1^{T}X_1)^{-1} =
(X^{T}X)^{-1}+(X^{T}X)^{-1}\boldsymbol{x}_1(1-\boldsymbol{x}_1^{T}(X^{T}X)^{-1}\boldsymbol{x}_1)^{-1}\boldsymbol{x}_1^{T}(X^{T}X)^{-1}.
\]
De manera análoga a \eqref{Prob7-Eq2}, tenemos que $X_1^{T}\boldsymbol{Y}_{(1)} = X^{T}\boldsymbol{Y} - \boldsymbol{x}_1Y_1$, de donde se sigue que
\[
\begin{split}
\hat{\boldsymbol{\beta}}_{(1)} & = \;(X_1^{T}X_1)^{-1}X_1^{T}\boldsymbol{Y}_{(1)}\\[5pt]
&= \; (X^{T}X)^{-1}X^{T}\boldsymbol{Y}+(X^{T}X)^{-1}\boldsymbol{x}_1\left[1-\boldsymbol{x}_1^{T}(X^{T}X)^{-1}\boldsymbol{x}_1\right]^{-1}\boldsymbol{x}_1^{T}(X^{T}X)^{-1}X^{T}\boldsymbol{Y} \\
    & \quad -(X^{T}X)^{-1}\boldsymbol{x}_1Y_1 -(X^{T}X)^{-1}\boldsymbol{x}_1\left[1-\boldsymbol{x}_1^{T}(X^{T}X)^{-1}\boldsymbol{x}_1\right]^{-1}\boldsymbol{x}_1^{T}(X^{T}X)^{-1}\boldsymbol{x}_1Y_1 \\[10pt]
& = \; \hat{\boldsymbol{\beta}}+(X^{T}X)^{-1}\boldsymbol{x}_1(1-h_{11})^{-1}\boldsymbol{x}_1^{T}\hat{\boldsymbol{\beta}}
    -(X^{T}X)^{-1}\boldsymbol{x}_1Y_1 -(X^{T}X)^{-1}\boldsymbol{x}_1(1-h_{11})^{-1}h_{11}Y_1 \\[5pt]
& = \; \hat{\boldsymbol{\beta}}+\frac{(X^{T}X)^{-1}}{1-h_{11}} \left[
     \boldsymbol{x}_1\hat{Y}_1 - (1-h_{11})\boldsymbol{x}_1Y_1 - \boldsymbol{x}_1h_{11}Y_1\right]\\
& = \; \hat{\boldsymbol{\beta}}-\frac{(X^{T}X)^{-1}}{1-h_{11}} \boldsymbol{x}_1e_1 \\[5pt]
& = \; \hat{\boldsymbol{\beta}}-(X^{T}X)^{-1}\boldsymbol{x}_1 \frac{e_1}{1-h_{11}}.
\end{split}
\]
Por consiguiente, como $i=1$ fue tomado sin pérdida de generalidad, se concluye que \eqref{Prob7-Eq1} es válida.

Por otra parte,
\[
D_i = \frac{\sum_{j=1}^{n} {\left(\hat{y}_j-\hat{y}_{j(i)}\right)}^2}{p\hat{\sigma}^2} =\frac{\left(\widehat{\boldsymbol{Y}}_{(i)}-\widehat{\boldsymbol{Y}}\right)^{T}\left(\widehat{\boldsymbol{Y}}_{(i)}-\widehat{\boldsymbol{Y}}\right)}{p\hat{\sigma}^2} = \frac{(\widehat{\boldsymbol{\beta}}_{(i)}-\widehat{\boldsymbol{\beta}})^{T}(X^{T}X)\; (\widehat{\boldsymbol{\beta}}_{(i)}-\widehat{\boldsymbol{\beta}})}{p\hat{\sigma}^2}.
\]
Por consiguiente,
\[
\begin{split}
  D_i&= \frac{1}{p\hat{\sigma}^2}(\widehat{\boldsymbol{\beta}}_{(i)}-\widehat{\boldsymbol{\beta}})^{T}(X^{T}X)(\widehat{\boldsymbol{\beta}}_{(i)}-\widehat{\boldsymbol{\beta}})\\
  &= \frac{1}{p\hat{\sigma}^2}\left(\frac{e_i}{1-h_{ii}}\right)^2\boldsymbol{x}_i^{T}(X^{T}X)^{-1}(X^{T}X)(X^{T}X)^{-1}\boldsymbol{x}_i\\
  &= \frac{1}{p\hat{\sigma}^2}\left(\frac{e_i}{1-h_{ii}}\right)^2\boldsymbol{x}_i^{T}(X^{T}X)^{-1}\boldsymbol{x}_i\\
  &= \frac{1}{p\hat{\sigma}^2}\left(\frac{e_i}{1-h_{ii}}\right)^2h_{ii}\\
  &= \left( \frac{e_i}{\hat{\sigma}\sqrt{(1-h_{ii})}} \right)^2\left(\frac{h_{ii}}{p\,(1-h_{ii})}\right)\\
  &= \frac{r_i^2}{p} \cdot \frac{h_{ii}}{1-h_{ii}}.
\end{split}
\]
La distancia de Cook con la expresión anterior tiene la siguiente interpretación: el término $r_i^2$ mide qué tan alejada se encuentra la observación $i$-ésima del resto de los datos, y esta se multiplica por el factor $\frac{h_{ii}}{1-h_{ii}}$, que mientras más grande la influencia de la observación $i$-ésima ($h_{ii}$), es mayor. Por consiguiente, esta distancia combina la influencia de la observación con su discrepancia con el resto de los datos, dándole mayor peso a observaciones con influencias mayores. Por último, el factor $\frac{1}{p}$ indica que, mientras más complejo es el modelo, las distancias de Cook serán menores, porque el espacio parametral es más amplio.
\end{soln}

\newpage
% Problema 8
\begin{soln}
Dado que $a>0$, se tiene que
\[
\min(y)\coloneqq \min_{1\leq i\leq n} y_i=\min_{1\leq i\leq n}(ax_i+b) = b+\min_{1\leq i\leq n}(ax_i) = b+a\min_{1\leq i\leq n} x_i = a\min(x)+b.
\]
De manera análoga,
\[
\max(y)\coloneqq \max_{1\leq i\leq n} y_i=\max_{1\leq i\leq n}(ax_i+b) = b+\max_{1\leq i\leq n}(ax_i) = b+a\max_{1\leq i\leq n} x_i =a\max(x)+b.
\]
Por consiguiente, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, se tiene que
\[
y_i^* = \frac{y_i-\min(y)}{\max(y)-\min(y)} = \frac{\left(ax_i+b\right)-\left(a\min(x)+b\right)}{\left(a\max(x)+b\right)-\left(a\min(x)+b\right)} = \frac{a\left(x_i-\min(x)\right)}{a\left(\max(x)-\min(x)\right)} = x_i^*,
\]
que es lo deseado.
\end{soln}

\newpage
% Problema 9
\begin{soln}
(a) Como el soporte de $X$ es $[x_m, \infty)$, con $x_m>0$, la transformación $Y=\log(X)$ está bien definida, y $Y$ tiene soporte en $[\log(x_m), \infty)$. Además, la función $g(x)=\log(x)$ definida en $\RR^{+}$ es uno a uno y tiene inversa $g^{-1}(y)=e^y$, la cual es una función derivable, con $\frac{d}{dy}{g^{-1}}(y)=e^y$. Por lo tanto, como se cumple la relación $Y=\log(X)$ y por consiguiente $X=e^Y$, por el Teorema de Cambio de Variables $Y$ tiene densidad dada por
\[
f_Y(y) = \left\lvert \frac{dx}{dy} \right\rvert f_X(e^y) \mathds{1}_{[\log(x_m), \infty)}(y) = e^y \frac{\alpha x_m^{\alpha}}{e^{y(\alpha+1)}}\mathds{1}_{[\log(x_m), \infty)}(y) = \alpha {\left(\frac{x_m}{e^y}\right)}^\alpha \mathds{1}_{[\log(x_m), \infty)}(y).
\]
Notemos que esta última expresión puede ser escrita como
\[
f_Y(y) = \alpha e^{-\alpha \left(y-\log(x_m)\right)}\mathds{1}_{[0, \infty)}(y-\log(x_m)),
\]
de donde se puede observar que $Y \stackrel{d}{=} \log(x_m)+Exp(\alpha)$, en donde $Exp(\alpha)$ es una variable aleatoria con distribución exponencial de media $\frac{1}{\alpha}$. En particular, de aquí se sigue que la función de distribución acumulada de $Y$ es
\[
F_Y(y) = \begin{cases}
  0, & \text{si } y<\log(x_m),\\
  1-e^{-\alpha(y-\log(x_m))}, & \text{si } y\geq \log(x_m).
\end{cases}
\]

(b) Primero veamos que, dado $x>x_m$,
\[
\probability{X\geq x} = \int_{x}^{\infty} \frac{\alpha x_m^{\alpha}}{t^{\alpha+1}} dt = x_m^{\alpha} {\left[-t^{-\alpha}\right]}_{t=x}^{\infty} = {\left(\frac{x_m}{x}\right)}^{\alpha}.
\]
De este modo, la cola de $X$ decae de forma polinomial, del orden $x^{-\alpha}$. Por otro lado, si $y>\log(x_m)$,
\[
\probability{Y\geq y} = e^{\alpha \log(x_m)}e^{-\alpha y} = x_m^{\alpha} e^{-\alpha y},
\]
de donde podemos ver que la cola de $Y$ decae de forma polinomial, del orden $e^{-\alpha y}$ (más rápidamente que el decaimiento polinomial). Es decir, $X$ tiene colas más pesadas, y al transformarse a $Y$, cambia a colas más ligeras.

(c) Notemos que, como $Y=\log(X)$, para todo $y\in\RR$ se cumple que
\[
\probability{Y>y} = \probability{X>e^y},
\]
de modo que, como $e^y$ crece más rápido que $y$, las colas de $Y$ decaen más rápidamente de las de $X$, como lo visto con la distribución Pareto, en donde un decaimiento polinomial se convierte en uno exponencial. Además, como la función logaritmo es creciente y $\log(x)\leq \log(x+1)\leq x$ para todo $x>0$, por lo general $Y$ tiene un soporte más grande que $X$.

Más aún, por las propiedades de la función logarítmica, los cambios grandes en $X$ se reflejan en cambios más chicos de $Y$. Por ejemplo, si un valor de $X$ se duplica, en la transformación logarítmica el valor de $Y$ solo incrementa en $\log 2$ (cambios multiplicativos se transforman en cambios aditivos). Por consiguiente, si $X$ tiene colas muy pesadas, $Y$ tiende a distribuir el peso a lo largo de los reales y no tan concentrado en las colas; es decir, se ``acortan'' las colas largas. Además esto produce, por lo general, distribuciones más cercanas a la simetría, en especial cuando hay errores multiplicativos, que se convierten en errores aditivos al aplicar logaritmo, y el Teorema del Límite Central explica dicha simetría.
\end{soln}

\newpage
% Problema 10
\begin{soln}
(a) La media $\overline{x}$ está dada por
\[
\overline{x} = \frac{1+2+3+4+M}{5} = 2+\frac{M}{5},
\]
mientras que la desviación estándar de la muestra es
\[
\begin{split}
  s = \sqrt{\sum_{i=1}^{5} {(x_i-\overline{x})}^2} &= \sqrt{{\left(1+\frac{M}{5}\right)}^2+{\left(\frac{M}{5}\right)}^2+{\left(1-\frac{M}{5}\right)}^2+{\left(2-\frac{M}{5}\right)}^2+{\left(2-\frac{4M}{5}\right)}^2}\\
  &= \sqrt{\frac{4}{5}M^2-4M+10}.
\end{split}
\]

(b) Dado que $M\to\infty$, los datos en $x$ están ordenados, y por consiguiente su mediana es $3$ (el de en medio).

Ahora bien, el cuartil $1$ ($Q_{0.25}$) es la mediana de $\left\lbrace 1, 2 \right\rbrace$, que es $\frac{1+2}{2}$. Por otro lado, el tercer cuartil ($Q_{0.75}$) es  la mediana de $\left\lbrace 4, M \right\rbrace$, que es $\frac{4+M}{2}$. Luego, el rango intercuartílico es
\[
RIQ = Q_{0.75}-Q_{0.25} = \frac{4+M}{2}-\frac{3}{2} = \frac{1+M}{2}.
\]

(c) Cuando se hace $M\to\infty$, la media, la desviación estándar y el rango intercuartílico tienden a $\infty$, pero la mediana permanece constante. Como interpretación, $M$ se convierte en un ``outlier'' cuando se vuelve suficientemente grande, y afecta las medidas de tendencia central y de dispersión mencionadas anteriormente, con excepción de la mediana, por lo que esta es más robusta que la media (no es tan sensible a valores extremos).
\end{soln}

\newpage
% Problema 11
\begin{soln}
(a) Sea $x>0$, y veamos que
\[
\lim_{\lambda\to 0} (x^{\lambda}-1) = 1-1=0,
\]
mientras que $\lim_{\lambda\to 0} \lambda = 0$. Además, la función $\lambda \mapsto \lambda$ es derivable, con derivada igual a $1(\neq 0)$. Por lo tanto, ya que el siguiente límite existe, por la Regla de l'Hôpital se tiene que
\[
\log(x) = \lim_{\lambda\to 0} \frac{\log(x) x^\lambda}{1} = \lim_{\lambda\to 0} \frac{x^{\lambda}-1}{\lambda}=\lim_{\lambda\to 0} y(\lambda).
\]

(b) Consideremos a la sucesión ${(x_n)}_{n\in\NN}$, en donde $x_n = 2^n$ para todo $n\in\NN$. Dicha sucesión toma valores muy dispersos cuando $n$ es muy grande, pues sus primeros valores son
\[
2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, \ldots.
\]
La sucesión correpondiente a la transformación de Box y Cox con $\lambda=1$ es la misma pero recorrida en $1$, así que sigue siendo igual de dispersa. Sin embargo, con la transformación logarítmica ($\lambda=0$), se convierte en ${(y_n)}_{n\in\NN}$, en donde $y_n = n\log(2)$ para todo $n\in\NN$, que es mucho menos dispersa. A manera de ilustración, sus primeros valores son aproximadamente iguales a:
\[
0.6931, 1.3863, 2.0794, 2.7726, 3.4657, 4.1589, 4.852, 5.5452, 6.2383, 6.9315, 7.6246, 8.3178, 9.0109, \ldots
\]
\end{soln}

\newpage
% Problema 12
\begin{soln}
a) Con las hipótesis del enunciado, observemos que la función $\hat{f}_h(x)$ es una suma de funciones indicadoras, que cuenta el número de observaciones $x_i$ que están en el mismo conjunto que $x$, $I_j$. Luego, ya que $1\{x_i \in I_j\} \geq 0$ y $nh>0$ se tiene que $\hat{f}_h(x)\geq 0$.

b) Para dar respuesta a este inciso, notemos que la función $\hat{f}_h(x)$ se puede escribir como
\[ 
\hat{f}_h(x)=\frac{1}{n h} \sum_{j=1}^k 1\{x \in I_j\}\sum_{i=1}^n 1\left\{x_i \in I_j\right\},
\]
en donde hemos considerado que los intervalos $I_1, \ldots, I_k$ son ajenos. Luego, la integral buscada se puede ver como
\[
\begin{split}
  & \int_{-\infty}^{\infty} \hat{f}_h(x) d x= \int_{-\infty}^{\infty} \frac{1}{n h} \sum_{j=1}^k 1\{x \in I_j\}\sum_{i=1}^n 1\left\{x_i \in I_j\right\} d x \\
  &=  \frac{1}{n h} \sum_{j=1}^k \int_{\left\lbrace x\in I_j \right\rbrace} \sum_{i=1}^n 1\left\{x_i \in I_j\right\} dx\\
  &=  \frac{1}{n h} \sum_{j=1}^k \sum_{i=1}^n 1\left\{x_i \in I_j\right\} \int_{\left\lbrace x\in I_j \right\rbrace}  dx\\
  &=  \frac{1}{n h} \sum_{j=1}^k \sum_{i=1}^n 1\left\{x_i \in I_j\right\} h\\
  &=  \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^k 1\left\{x_i \in I_j\right\}\\ 
  &=  \frac{1}{n} \sum_{i=1}^n 1\\ 
  &=  \frac{n}{n}=1,
\end{split}
\]
en donde la igualdad en el penúltimo renglón se tiene ya que cada $x_i$ pertenece a algún conjunto $I_j$.

c) Observemos que cuando $h$ es grande, los intervalos contendrán más datos, esto nos llevará a que no se aprecie si hay algún patrón en el comportamiento de los datos, es decir si los datos tienen preferencia por ciertos intervalos. Esto tiene la ventaja que la varianza es menor, pero el sesgo es mayor. Por otro lado, cuando $h$ es muy pequeño, los intervalos no alcanzarán a contener muchos datos, lo cual hace que haya un sesgo menor, pero mayor varianza; es decir, un sobreajuste. Un caso extremo de ver esto es hacer a $h$ muy cercano a cero de tal forma que cada intervalo contenga a lo más un dato, y en este caso, solo se verán barras de la misma altura alrededor de cada dato.

\end{soln}

\newpage
% Problema 13
\begin{soln}
\textbf{Normalización)} Con las hipótesis del enunciado, observemos que la integral se puede escribir como
\[
\begin{split}
  & \int_{-\infty}^{\infty} \hat{f}_h(x) d x=\int_{-\infty}^{\infty} \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x-x_i}{h}\right) d x \\
  &=  \frac{1}{n h}\sum_{i=1}^n  \int_{-\infty}^{\infty} K\left(\frac{x-x_i}{h}\right) d x \\
  &= \frac{1}{n h}\sum_{i=1}^n  \int_{-\infty}^{\infty} K(u) h d u \\
  &=  \frac{1}{n}\sum_{i=1}^n  \int_{-\infty}^{\infty} K(u) d u \\
  &=  \frac{1}{n}\sum_{i=1}^n 1=1,
\end{split}
\]
en donde hemos usado el Teorema de Fubini para el intercambio de la suma y la integral para la segunda igualdad, y la tercera igualdad se tiene haciendo el cambio de variables $u=\frac{x-x_i}{h}$. Además, se ha considerado que $K$ integra $1$ para la primera igualdad del último renglón.

\textbf{No negatividad)} Para este inciso basta observar por hipótesis $K(u) \geq 0$ y que $nh>0$, y como la integral de funciones no negativas es no negativas, se concluye que $\hat{f}_h(x)\geq 0$.

\textbf{Sesgo puntual)} Observemos que, como la muestra $x_1, \ldots, x_n$ corresponde a variables aleatorias iid, el sesgo puntual se puede escribir como
\[
\begin{split}
  & \mathbb{E}\left[\hat{f}_h(x)\right]-f(x)=\mathbb{E}\left[\sum_{i=1}^n \frac{1}{n h} K\left(\frac{x-x_i}{h}\right)\right]-f(x) \\
  &=  \frac{1}{n h} \sum_{i=1}^n \mathbb{E}\left[K\left(\frac{x-x_i}{h}\right)\right]-f(x) \\
  &=  \frac{1}{h} \mathbb{E}\left[K\left(\frac{x-x_1}{h}\right)\right]-f(x) \\
  &=  \frac{1}{h} \int_{-\infty}^{\infty} K\left(\frac{x-x_1}{h}\right) f\left(x_1\right) d x_1-f(x).
\end{split}
\] 
Haciendo el cambio de variables $u=\frac{x-x_1}{h}$ y considerando que la integral del kernel es igual a uno, se tiene que la expresión anterior es igual a
\begin{equation}\label{Prob13-Eq1}
  \begin{split}
  \mathbb{E}\left[\hat{f}_h(x)\right]-f(x) &= \frac{1}{h} \int_{-\infty}^{\infty} h K(u) f(x-u h) d u-\int_{-\infty}^{\infty} K(u) f(x) d u \\
  & = \int_{-\infty}^{\infty} K(u)[f(x-u h)-f(x)] d u.
\end{split}
\end{equation}
Por otro lado, asumiendo que $f$ tiene derivadas de segundo orden, por el Teorema de Taylor con errores de Peano, se tiene la siguiente aproximación de orden dos de $f(x-uh)$ alrededor de $x$:
\[
f(x-u h)=f(x)+f^{\prime}(x)(x-u h-x)+\frac{f^{\prime \prime}(x){(x-u h-x)}^2}{2!}+h_2(x-uh){(uh)}^2,
\]
en donde $h_2$ es una función tal que
\[
\lim_{h\to 0}h_2(x-uh)=0.
\]
Sustituyendo lo anterior en \eqref{Prob13-Eq1} se tiene que
\[
\begin{split}
  \mathbb{E}\left[\hat{f}_h(x)\right]-f(x) &= \int_{-\infty}^{\infty} K(u)\left[f^{\prime}(x)(-u h)+\frac{f^{\prime \prime}(x){(u h)}^2}{2!}+h_2(x-uh){(uh)}^2\right] d u \\
  &=  \frac{f^{\prime \prime}(x)}{2!} h^2 \int_{-\infty}^{\infty} u^2 K(u) d u+h^2 \int_{-\infty}^{\infty} K(u) h_2(x-uh) u^2  d u,
\end{split}
\]
donde esta última igualdad se tiene ya que $\int uK(u)du=0$ y $\mu_2(K)$ es finito. Además, ya que $h_2$ está acotada alrededor de $x$ (es decir, cuando $h$ es pequeño), entonces para $h$ suficientemente chico se tiene que
\[
\left\lvert h_2(x-uh) \right\rvert \leq 1,
\]
lo que implica que
\[
\left\lvert K(u) h_2(x-uh) u^2 \right\rvert \leq \left\lvert u^2 K(u) \right\rvert,
\]
y dicha función es integrable por hipótesis, pues $\mu_2(K)$ existe y es finito. Luego, por el Teorema de Convergencia Dominada se tiene que
\[
\lim_{h\to 0} \int_{-\infty}^{\infty} K(u) h_2(x-uh) u^2  d u = \int_{-\infty}^{\infty} \lim_{h\to 0} K(u) h_2(x-uh) u^2  d u = 0.
\]
Por lo tanto,
\[
h^2 \int_{-\infty}^{\infty} K(u) h_2(x-uh) u^2  d u = o(h^2).
\]
En conclusión,
\[
\mathbb{E}\left[\hat{f}_h(x)\right]-f(x) = \frac{h^2}{2}\mu_2(K)f''(x)+o(h^2).
\]
\end{soln}

\end{document}