\documentclass[twoside,12pt]{article}

\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 }

\usepackage[spanish,activeacute,es-noindentfirst]{babel} %paqueteria para idioma
\spanishdecimal{.}
\usepackage{pstricks}
\usepackage[utf8]{inputenc}
%\usepackage{pstricks-add}


%Para Figuras en Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}

\usepackage{amssymb,amsfonts,amsbsy,amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{float}
% \usepackage{enumitem}
\usepackage{enumerate} %Para poner \begin{enumerate}[a)]
\usepackage[hidelinks]{hyperref}

\usepackage{subfig}

\usepackage{rotating} %para rotar texto en tablas con \begin{sideways}
%\end{sideways}
\usepackage{makecell} %Para dividir celdas de tablas en varias l\'ineas


\theoremstyle{definition}
\newtheorem{probn}{Problema}
\newtheorem{soln}{Solución del problema}

\title{\textsc{Tarea 1\\ {\Large Ciencia de Datos}}}
\author{\large{\textbf{Brain de Jesús Salazar, César Ávila, Iván García}}}
\date{12 de septiembre de 2025}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\footrulewidth}{0.1pt}
\fancyhead[LE]{\footnotesize{\textsc{Tarea 1}}}
\fancyhead[RO]{\footnotesize{\textsc{Ciencia de Datos}}}
\rfoot{\textsc{\thepage}}
\lfoot{\footnotesize{\textsc{Brain de Jesús Salazar, César Ávila, Iván García}}}

\usepackage{listings}

\def\sin{\mathop{\mathrm{\normalfont sen}}\nolimits}
\def\cos{\mathop{\mathrm{\normalfont cos}}\nolimits}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\XX}{\boldsymbol{X}}
\newcommand{\YY}{\boldsymbol{Y}}
\newcommand{\ZZ}{\boldsymbol{Z}}
\newcommand{\variance}{\mathrm{\normalfont Var}}
\newcommand{\covariance}{\mathrm{\normalfont Cov}}
\newcommand{\probability}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\expectation}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\given}{\,\middle|\,}

\begin{document}
\maketitle

% Problema 1
\begin{soln}

\end{soln}

\newpage
% Problema 2
\begin{soln}

\end{soln}

\newpage
% Problema 3
\begin{soln}

\end{soln}

\newpage
% Problema 4
\begin{soln}

\end{soln}

\newpage
% Problema 5
\begin{soln}
Consideremos una muestra $Y_1, \ldots, Y_n$ de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza $\sigma^2$, con indicadores $R_i \in \left\lbrace 0, 1 \right\rbrace$, de tal manera que $R_i\perp Y_i$ para cada $i\in \left\lbrace 1, \ldots, n \right\rbrace$. Dichas $R_i$ existen pues estamos bajo el modelo MCAR, y se interpretan como $R_i = 1$ si y solo si el dato $Y_i$ fue observado.

Notemos que si $n_{obs}$ representa el número de datos observados, entonces $n_{obs} = \sum_{i=1}^{n} R_i$. Además, por la definición de los $R_i$,
\[
\overline{Y}_{obs} = \frac{1}{n_{obs}} \sum_{i=1}^{n} R_i Y_i = \frac{1}{n_{obs}} \sum_{i: R_i=1} Y_i.
\]
Así pues, si $\boldsymbol{R}=(R_1, \ldots, R_n)$, entonces
\[
\begin{split}
  \expectation{\overline{Y}_{obs} \given \boldsymbol{R}} = \expectation{\frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i Y_i  \given \boldsymbol{R}} &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} \expectation{R_i Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i}\\
  &=\mu,
\end{split}
\]
en donde hemos usado que las $Y_i$ son iid con media $\mu$ y son independientes de $\boldsymbol{R}$ (por las hipótesis del modelo MCAR). Por consiguiente,
\[
\expectation{\overline{Y}_{obs}} = \expectation{\expectation{\overline{Y}_{obs} \given \boldsymbol{R}}} = \mu.
\]
Por otra parte,
\[
\overline{Y}_{obs}^2 = \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n}R_i^2 Y_i^2 + \sum_{i\neq j}R_i R_j Y_i Y_j\right],
\]
por lo que
\[
\begin{split}
  \expectation{\overline{Y}_{obs}^2 \given \boldsymbol{R}} &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2 \given \boldsymbol{R}} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j \given \boldsymbol{R}}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[(\sigma^2+\mu^2)\sum_{i=1}^{n} R_i^2 + \mu^2\sum_{i\neq j}R_i R_j\right]\\
  &= \mu^2+\sigma^2\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}.
\end{split}
\]
De lo anterior se sigue que
\[
\begin{split}
  \variance \left[\overline{Y}_{obs}\right] = \expectation{\overline{Y}_{obs}^2}-{\left(\expectation{\overline{Y}_{obs}}\right)}^2 &= \expectation{\expectation{\overline{Y}_{obs}^2\given \boldsymbol{R}}}-\mu^2
  = \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{1}{\sum_{i=1}^{n} R_i}}\\
  &= \sigma^2 \expectation{\frac{1}{n_{obs}}}\\
  &\geq \frac{\sigma^2}{n} = \variance \left[\ \overline{Y}\ \right],
\end{split}
\]
en donde hemos usado que $R_i^2 = R_i$, pues $R_i\in \left\lbrace 0, 1 \right\rbrace$, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, y que $n_{obs}\leq n$. De hecho, siguiendo un procedimiento completamente análogo, pero ahora con varianzas condicionales, se sigue que
\[
\variance \left[\overline{Y}_{obs} \given \boldsymbol{R}\right] = \frac{\sigma^2}{n_{obs}}.
\]
De lo anterior podemos notar que $\overline{Y}_{obs}$ es insesgado, pero que $\variance \left[\overline{Y}_{obs}\right] \geq \variance \left[\ \overline{Y}\ \right]$, por lo que $\overline{Y}_{obs}$ tiene menor eficiencia (posee más varianza, pues la eliminación de datos hace que haya menos de ellos para poder estimar a la media de $Y$).
\end{soln}

\newpage
% Problema 6
\begin{soln}
Sean $\YY = (Y_{obs}, Y_{mis})$ y $\boldsymbol{R}$ el patrón de datos faltantes. Bajo la definición del MAR,
\[
\probability{\boldsymbol{R} \given Y_{obs}, Y_{mis}, \theta, \psi} = \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Así pues, bajo este modelo,
\[
\probability{\YY, \boldsymbol{R} \given \theta, \psi} = \probability{\YY \given \theta} \probability{\boldsymbol{R} \given \YY, \psi} =  \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Por consiguiente, la verosimilitud de $\theta$ está dada por
\[
\begin{split}
  L\left(\theta; Y_{obs}, \boldsymbol{R}\right) = \int \probability{\YY, \boldsymbol{R} \given \theta, \psi} dY_{mis} &= \int \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \int \probability{\YY \given \theta} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta}.
\end{split}
\]
Ya que el factor $\probability{\boldsymbol{R} \given Y_{obs}, \psi}$ no depende de $\theta$, se sigue que $L\left(\theta; Y_{obs}, \boldsymbol{R}\right) \propto \probability{Y_{obs} \given \theta}$. Para ver las condiciones \textit{a priori} que garantizan ignorabilidad bajo el enfoque bayesiano, notemos que
\[
\begin{split}
  \probability{\theta \given Y_{obs}, \boldsymbol{R}} = \int \probability{\theta, \psi \given Y_{obs}, \boldsymbol{R}} d\psi &\propto \int \probability{Y_{obs}, \boldsymbol{R} \given \theta, \psi} \pi\left(\theta, \psi\right) d\psi\\
  &= \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta} \pi\left(\theta, \psi\right) d\psi\\
  &= \probability{Y_{obs} \given \theta} \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \pi\left(\theta, \psi\right) d\psi.
\end{split}
\]
Para concluir ignorabilidad buscamos que $L\left(\theta \given Y_{obs}, \boldsymbol{R}\right) \propto \pi(\theta) \probability{Y_{obs} \given \theta}$, y ya que la integral anterior depende de $\theta$ solamente a través del factor $\pi\left(\theta, \psi\right)$, dicha ignorabilidad se logra cuando $\pi(\theta, \psi)=\pi(\theta)\pi(\psi)$. Es decir, si en la \textit{a priori} se pide indistinguibilidad de los parámetros (i.e. que $\theta$ y $\psi$ sean independientes), entonces el mecanismo es ignorable para inferir $\theta$.
\end{soln}

\newpage
% Problema 7
\begin{soln}

\end{soln}

\newpage
% Problema 8
\begin{soln}
Dado que $a>0$, se tiene que
\[
\min(y)\coloneqq \min_{1\leq i\leq n} y_i=\min_{1\leq i\leq n}(ax_i+b) = b+\min_{1\leq i\leq n}(ax_i) = b+a\min_{1\leq i\leq n} x_i = a\min(x)+b.
\]
De manera análoga,
\[
\max(y)\coloneqq \max_{1\leq i\leq n} y_i=\max_{1\leq i\leq n}(ax_i+b) = b+\max_{1\leq i\leq n}(ax_i) = b+a\max_{1\leq i\leq n} x_i =a\max(x)+b.
\]
Por consiguiente, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, se tiene que
\[
y_i^* = \frac{y_i-\min(y)}{\max(y)-\min(y)} = \frac{\left(ax_i+b\right)-\left(a\min(x)+b\right)}{\left(a\max(x)+b\right)-\left(a\min(x)+b\right)} = \frac{a\left(x_i-\min(x)\right)}{a\left(\max(x)-\min(x)\right)} = x_i^*,
\]
que es lo deseado.
\end{soln}

\newpage
% Problema 9
\begin{soln}
(a) Como el soporte de $X$ es $[x_m, \infty)$, con $x_m>0$, la transformación $Y=\log(X)$ está bien definida, y $Y$ tiene soporte en $[\log(x_m), \infty)$. Además, la función $g(x)=\log(x)$ definida en $\RR^{+}$ es uno a uno y tiene inversa $g^{-1}(y)=e^y$, la cual es una función derivable, con $\frac{d}{dy}{g^{-1}}(y)=e^y$. Por lo tanto, como se cumple la relación $Y=\log(X)$ y por consiguiente $X=e^Y$, por el Teorema de Cambio de Variables $Y$ tiene densidad dada por
\[
f_Y(y) = \left\lvert \frac{dx}{dy} \right\rvert f_X(e^y) \mathds{1}_{[\log(x_m), \infty)}(y) = e^y \frac{\alpha x_m^{\alpha}}{e^{y(\alpha+1)}}\mathds{1}_{[\log(x_m), \infty)}(y) = \alpha {\left(\frac{x_m}{e^y}\right)}^\alpha \mathds{1}_{[\log(x_m), \infty)}(y).
\]
Notemos que esta última expresión puede ser escrita como
\[
f_Y(y) = \alpha e^{-\alpha \left(y-\log(x_m)\right)}\mathds{1}_{[0, \infty)}(y-\log(x_m)),
\]
de donde se puede observar que $Y \stackrel{d}{=} \log(x_m)+Exp(\alpha)$, en donde $Exp(\alpha)$ es una variable aleatoria con distribución exponencial de media $\frac{1}{\alpha}$. En particular, de aquí se sigue que la función de distribución acumulada de $Y$ es
\[
F_Y(y) = \begin{cases}
  0, & \text{si } y<\log(x_m),\\
  1-e^{-\alpha(y-\log(x_m))}, & \text{si } y\geq \log(x_m).
\end{cases}
\]

(b) Primero veamos que, dado $x>x_m$,
\[
\probability{X\geq x} = \int_{x}^{\infty} \frac{\alpha x_m^{\alpha}}{t^{\alpha+1}} dt = x_m^{\alpha} {\left[-t^{-\alpha}\right]}_{t=x}^{\infty} = {\left(\frac{x_m}{x}\right)}^{\alpha}.
\]
De este modo, la cola de $X$ decae de forma polinomial, del orden $x^{-\alpha}$. Por otro lado, si $y>\log(x_m)$,
\[
\probability{Y\geq y} = e^{\alpha \log(x_m)}e^{-\alpha y} = x_m^{\alpha} e^{-\alpha y},
\]
de donde podemos ver que la cola de $Y$ decae de forma polinomial, del orden $e^{-\alpha y}$ (más rápidamente que el decaimiento polinomial). Es decir, $X$ tiene colas más pesadas, y al transformarse a $Y$, cambia a colas más ligeras.

(c) Notemos que, como $Y=\log(X)$, para todo $y\in\RR$ se cumple que
\[
\probability{Y>y} = \probability{X>e^y},
\]
de modo que, como $e^y$ crece más rápido que $y$, las colas de $Y$ decaen más rápidamente de las de $X$, como lo visto con la distribución Pareto, en donde un decaimiento polinomial se convierte en uno exponencial. Además, como la función logaritmo es creciente y $\log(x)\leq \log(x+1)\leq x$ para todo $x>0$, por lo general $Y$ tiene un soporte más grande que $X$.

Más aún, por las propiedades de la función logarítmica, los cambios grandes en $X$ se reflejan en cambios más chicos de $Y$. Por ejemplo, si un valor de $X$ se duplica, en la transformación logarítmica el valor de $Y$ solo incrementa en $\log 2$ (cambios multiplicativos se transforman en cambios aditivos). Por consiguiente, si $X$ tiene colas muy pesadas, $Y$ tiende a distribuir el peso a lo largo de los reales y no tan concentrado en las colas; es decir, se ``acortan'' las colas largas. Además esto produce, por lo general, distribuciones más cercanas a la simetría, en especial cuando hay errores multiplicativos, que se convierten en errores aditivos al aplicar logaritmo, y el Teorema del Límite Central explica dicha simetría.
\end{soln}

\newpage
% Problema 10
\begin{soln}

\end{soln}

\newpage
% Problema 11
\begin{soln}
(a) Sea $x>0$, y veamos que
\[
\lim_{\lambda\to 0} (x^{\lambda}-1) = 1-1=0,
\]
mientras que $\lim_{\lambda\to 0} \lambda = 0$. Además, la función $\lambda \mapsto \lambda$ es derivable, con derivada igual a $1(\neq 0)$. Por lo tanto, ya que el siguiente límite existe, por la Regla de l'Hôpital se tiene que
\[
\log(x) = \lim_{\lambda\to 0} \frac{\log(x) x^\lambda}{1} = \lim_{\lambda\to 0} \frac{x^{\lambda}-1}{\lambda}=\lim_{\lambda\to 0} y(\lambda).
\]

(b) Consideremos a la sucesión ${(x_n)}_{n\in\NN}$, en donde $x_n = 2^n$ para todo $n\in\NN$. Dicha sucesión toma valores muy dispersos cuando $n$ es muy grande, pues sus primeros valores son
\[
2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, \ldots.
\]
La sucesión correpondiente a la transformación de Box y Cox con $\lambda=1$ es la misma pero recorrida en $1$, así que sigue siendo igual de dispersa. Sin embargo, con la transformación logarítmica ($\lambda=0$), se convierte en ${(y_n)}_{n\in\NN}$, en donde $y_n = n\log(2)$ para todo $n\in\NN$, que es mucho menos dispersa. A manera de ilustración, sus primeros valores son aproximadamente iguales a:
\[
0.6931, 1.3863, 2.0794, 2.7726, 3.4657, 4.1589, 4.852, 5.5452, 6.2383, 6.9315, 7.6246, 8.3178, 9.0109, \ldots
\]
\end{soln}

\newpage
% Problema 12
\begin{soln}

\end{soln}

\newpage
% Problema 13
\begin{soln}

\end{soln}

\end{document}