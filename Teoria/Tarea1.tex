\documentclass[twoside,12pt]{article}

\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 }

\usepackage[spanish,activeacute,es-noindentfirst]{babel} %paqueteria para idioma
\spanishdecimal{.}
\usepackage{pstricks}
\usepackage[utf8]{inputenc}
%\usepackage{pstricks-add}


%Para Figuras en Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}

\usepackage{amssymb,amsfonts,amsbsy,amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{float}
% \usepackage{enumitem}
\usepackage{enumerate} %Para poner \begin{enumerate}[a)]
\usepackage[hidelinks]{hyperref}

\usepackage{subfig}

\usepackage{rotating} %para rotar texto en tablas con \begin{sideways}
%\end{sideways}
\usepackage{makecell} %Para dividir celdas de tablas en varias l\'ineas


\theoremstyle{definition}
\newtheorem{probn}{Problema}
\newtheorem{soln}{Solución del problema}

\title{\textsc{Tarea 1\\ {\Large Ciencia de Datos}}}
\author{\large{\textbf{Brain de Jesús Salazar, César Ávila, Iván García}}}
\date{12 de septiembre de 2025}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\footrulewidth}{0.1pt}
\fancyhead[LE]{\footnotesize{\textsc{Tarea 1}}}
\fancyhead[RO]{\footnotesize{\textsc{Ciencia de Datos}}}
\rfoot{\textsc{\thepage}}
\lfoot{\footnotesize{\textsc{Brain de Jesús Salazar, César Ávila, Iván García}}}

\usepackage{listings}

\def\sin{\mathop{\mathrm{\normalfont sen}}\nolimits}
\def\cos{\mathop{\mathrm{\normalfont cos}}\nolimits}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\XX}{\boldsymbol{X}}
\newcommand{\YY}{\boldsymbol{Y}}
\newcommand{\ZZ}{\boldsymbol{Z}}
\newcommand{\variance}{\mathrm{\normalfont Var}}
\newcommand{\covariance}{\mathrm{\normalfont Cov}}
\newcommand{\probability}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\expectation}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\given}{\,\middle|\,}

\begin{document}
\maketitle

% Problema 1

\begin{soln}
\end{soln}


\newpage
% Problema 2
\begin{soln}
Considere el modelo de regresión lineal $\boldsymbol{Y}=X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ donde $\boldsymbol{\varepsilon}\sim N_n(\boldsymbol{0},\sigma^2I).$ Del ejercicio anterior se sabe que la matriz 
    $$
    H=X(X^TX)^{-1}X^T,
    $$
    es una matriz de proyección ortogonal. Usando la descomposición espectral, se puede descomponer a $H$ de la forma 
    $$H=RR^T,$$
    basta con reescribir a la matriz diagonal como el producto de dos matrices donde cada matriz tiene en su diagonal a la raíz cuadrada de cada elemento. Por otro lado, observe que ya que $H$ es idempotente se tiene que
    $$
    H^2=(RR^T)(RR^T)=H=RR^T,
    $$
    por lo que $$R^TR=I_p.$$ 
    Por otro lado, note que por la propiedad cíclica de la traza
    $$
    tr(H)=tr(RR^T)=tr(R^TR)=tr(I_p)=p.
    $$
    Por lo tanto, $$
\sum_{i=1}^n h_{i i}=p.
$$
\end{soln}

\newpage
% Problema 3
\begin{soln}
 Considere el modelo de regresión lineal clásico, $\boldsymbol{Y}=X\boldsymbol{\beta} + \boldsymbol{\varepsilon}$ donde $\boldsymbol{\varepsilon}\sim N_n(\boldsymbol{0},\sigma^2I)$ y a la matriz de proyección ortogonal,
    $$
    H=X(X^TX)^{-1}X^T.
    $$
 Se sabe que el vector de residuos se puede expresa como 
 $$
 \boldsymbol{e}=(I-H)\boldsymbol{Y},
 $$
luego ya que $\boldsymbol{Y}$ sigue una distribución normal multivariada con media $X\boldsymbol{\beta}$ y varianza $\sigma^2 I_n$, se tiene que, 
$$
\boldsymbol{e}\sim N_n\left(\mathbf{0}, \sigma^2\left(I_n-H\right)\right).
$$
De lo anterior, se tiene que 
$$
e_i \sim N(0,\sigma^2 (1-h_{ii}).
$$
y normalizando,
$$
\frac{e_i}{\sigma\sqrt{1-h_{ii}}}\sim N(0,1)
$$
Por otro lado, observe que si se considera a el estimador insesgado de la varianza,
$$\hat{\sigma}^2=\frac{\mathbf{e}^{\top} \mathbf{e}}{n-p}=\frac{\sigma^2 \mathbf{Y}^{\top}(I-H) \mathbf{Y}}{\sigma^2 (n-p)},$$
y se observa que $(I-H)$ es una matriz idempotente de rango $n-p$ se tiene que 
$$
\frac{(n-p)}{\sigma^2}\hat{\sigma}^2= \frac{ \mathbf{Y}^{\top}(I-H) \mathbf{Y}}{\sigma^2} \sim \chi^2(n-p).
$$
Por ultimo, se sabe que una variable aleatoria t de Student con r grados de libertad se define como la razón entre una variable aleatoria normal estándar sobre la raiz cuadrada de una variable chi cuadrada dividida entre sus r grados, con las variables aleatorias independientes. Ya que en este caso la normal estandar no es independiente de la chi cuadra se tiene que la siguiente variable aleatoria cumple que aproximadamente,

$$
\frac{\frac{e_i}{\sigma\sqrt{1-h_{ii}}}}{\sqrt{\frac{(n-p)}{(n-p)\sigma^2}\hat{\sigma}^2}} \sim t(n-p).
$$
Simplificando la expresión de la izquierda se tiene, 
$$
r_i=\frac{e_i}{\hat{\sigma} \sqrt{1-h_{i i}}} \sim t(n-p).
$$
Para arreglar el problema de la independencia se propone estimar a $\sigma^2$ sin usar el i-esimo dato, es decir

$$
\hat{\sigma}_{i}^2=\frac{\mathbf{e_i}^{\top} \mathbf{e_i}}{n-1-p}=\frac{\sigma^2 \mathbf{Y}_i^{\top}(I-H_i) \mathbf{Y_i}}{\sigma^2 (n-1-p)},
$$
donde el vector $\mathbf{Y}_i$ se obtiene eliminando al i-esima entrada y $H_i$ se construye eliminando la i-esíma entrada de $X.$ En este caso la matriz $(I-H_i)$ es idempotente de rango n-1-p, por lo que, 
$$
\frac{(n-1-p)}{\sigma^2}\hat{\sigma_i}^2= \frac{ \mathbf{Y_i}^{\top}(I-H_i) \mathbf{Y}_i}{\sigma^2} \sim \chi^2(n-1-p).
$$
Como en este caso $\hat{\sigma}_i$ no depende de $e_i$ se tiene que es independiente de la normal estandar definida previamente, de aquí se cumple que, 

$$
\frac{\frac{e_i}{\sigma\sqrt{1-h_{ii}}}}{\sqrt{\frac{(n-1-p)}{(n-1-p)\sigma^2}\hat{\sigma_i}^2}} \sim t(n-1-p).
$$
Simplificando el lado derecho se tiene que,
$$
r_i=\frac{e_i}{\hat{\sigma}_i \sqrt{1-h_{i i}}}\sim t(n-p-1).
$$
\end{soln}

\newpage
% Problema 4
\begin{soln}

\end{soln}

\newpage
% Problema 5
\begin{soln}
Consideremos una muestra $Y_1, \ldots, Y_n$ de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza $\sigma^2$, con indicadores $R_i \in \left\lbrace 0, 1 \right\rbrace$, de tal manera que $R_i\perp Y_i$ para cada $i\in \left\lbrace 1, \ldots, n \right\rbrace$. Dichas $R_i$ existen pues estamos bajo el modelo MCAR, y se interpretan como $R_i = 1$ si y solo si el dato $Y_i$ fue observado.

Notemos que si $n_{obs}$ representa el número de datos observados, entonces $n_{obs} = \sum_{i=1}^{n} R_i$. Además, por la definición de los $R_i$,
\[
\overline{Y}_{obs} = \frac{1}{n_{obs}} \sum_{i=1}^{n} R_i Y_i = \frac{1}{n_{obs}} \sum_{i: R_i=1} Y_i.
\]
Así pues, si $\boldsymbol{R}=(R_1, \ldots, R_n)$, entonces
\[
\begin{split}
  \expectation{\overline{Y}_{obs} \given \boldsymbol{R}} = \expectation{\frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i Y_i  \given \boldsymbol{R}} &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} \expectation{R_i Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i}\\
  &=\mu,
\end{split}
\]
en donde hemos usado que las $Y_i$ son iid con media $\mu$ y son independientes de $\boldsymbol{R}$ (por las hipótesis del modelo MCAR). Por consiguiente,
\[
\expectation{\overline{Y}_{obs}} = \expectation{\expectation{\overline{Y}_{obs} \given \boldsymbol{R}}} = \mu.
\]
Por otra parte,
\[
\overline{Y}_{obs}^2 = \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n}R_i^2 Y_i^2 + \sum_{i\neq j}R_i R_j Y_i Y_j\right],
\]
por lo que
\[
\begin{split}
  \expectation{\overline{Y}_{obs}^2 \given \boldsymbol{R}} &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2 \given \boldsymbol{R}} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j \given \boldsymbol{R}}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[(\sigma^2+\mu^2)\sum_{i=1}^{n} R_i^2 + \mu^2\sum_{i\neq j}R_i R_j\right]\\
  &= \mu^2+\sigma^2\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}.
\end{split}
\]
De lo anterior se sigue que
\[
\begin{split}
  \variance \left[\overline{Y}_{obs}\right] = \expectation{\overline{Y}_{obs}^2}-{\left(\expectation{\overline{Y}_{obs}}\right)}^2 &= \expectation{\expectation{\overline{Y}_{obs}^2\given \boldsymbol{R}}}-\mu^2
  = \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{1}{\sum_{i=1}^{n} R_i}}\\
  &= \sigma^2 \expectation{\frac{1}{n_{obs}}}\\
  &\geq \frac{\sigma^2}{n} = \variance \left[\ \overline{Y}\ \right],
\end{split}
\]
en donde hemos usado que $R_i^2 = R_i$, pues $R_i\in \left\lbrace 0, 1 \right\rbrace$, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, y que $n_{obs}\leq n$. De hecho, siguiendo un procedimiento completamente análogo, pero ahora con varianzas condicionales, se sigue que
\[
\variance \left[\overline{Y}_{obs} \given \boldsymbol{R}\right] = \frac{\sigma^2}{n_{obs}}.
\]
De lo anterior podemos notar que $\overline{Y}_{obs}$ es insesgado, pero que $\variance \left[\overline{Y}_{obs}\right] \geq \variance \left[\ \overline{Y}\ \right]$, por lo que $\overline{Y}_{obs}$ tiene menor eficiencia (posee más varianza, pues la eliminación de datos hace que haya menos de ellos para poder estimar a la media de $Y$).
\end{soln}

\newpage
% Problema 6
\begin{soln}
Sean $\YY = (Y_{obs}, Y_{mis})$ y $\boldsymbol{R}$ el patrón de datos faltantes. Bajo la definición del MAR,
\[
\probability{\boldsymbol{R} \given Y_{obs}, Y_{mis}, \theta, \psi} = \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Así pues, bajo este modelo,
\[
\probability{\YY, \boldsymbol{R} \given \theta, \psi} = \probability{\YY \given \theta} \probability{\boldsymbol{R} \given \YY, \psi} =  \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Por consiguiente, la verosimilitud de $\theta$ está dada por
\[
\begin{split}
  L\left(\theta; Y_{obs}, \boldsymbol{R}\right) = \int \probability{\YY, \boldsymbol{R} \given \theta, \psi} dY_{mis} &= \int \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \int \probability{\YY \given \theta} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta}.
\end{split}
\]
Ya que el factor $\probability{\boldsymbol{R} \given Y_{obs}, \psi}$ no depende de $\theta$, se sigue que $L\left(\theta; Y_{obs}, \boldsymbol{R}\right) \propto \probability{Y_{obs} \given \theta}$. Para ver las condiciones \textit{a priori} que garantizan ignorabilidad bajo el enfoque bayesiano, notemos que
\[
\begin{split}
  \probability{\theta \given Y_{obs}, \boldsymbol{R}} = \int \probability{\theta, \psi \given Y_{obs}, \boldsymbol{R}} d\psi &\propto \int \probability{Y_{obs}, \boldsymbol{R} \given \theta, \psi} \pi\left(\theta, \psi\right) d\psi\\
  &= \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta} \pi\left(\theta, \psi\right) d\psi\\
  &= \probability{Y_{obs} \given \theta} \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \pi\left(\theta, \psi\right) d\psi.
\end{split}
\]
Para concluir ignorabilidad buscamos que $L\left(\theta \given Y_{obs}, \boldsymbol{R}\right) \propto \pi(\theta) \probability{Y_{obs} \given \theta}$, y ya que la integral anterior depende de $\theta$ solamente a través del factor $\pi\left(\theta, \psi\right)$, dicha ignorabilidad se logra cuando $\pi(\theta, \psi)=\pi(\theta)\pi(\psi)$. Es decir, si en la \textit{a priori} se pide indistinguibilidad de los parámetros (i.e. que $\theta$ y $\psi$ sean independientes), entonces el mecanismo es ignorable para inferir $\theta$.
\end{soln}

\newpage
% Problema 7
\begin{soln}

\end{soln}

\newpage
% Problema 8
\begin{soln}
Dado que $a>0$, se tiene que
\[
\min(y)\coloneqq \min_{1\leq i\leq n} y_i=\min_{1\leq i\leq n}(ax_i+b) = b+\min_{1\leq i\leq n}(ax_i) = b+a\min_{1\leq i\leq n} x_i = a\min(x)+b.
\]
De manera análoga,
\[
\max(y)\coloneqq \max_{1\leq i\leq n} y_i=\max_{1\leq i\leq n}(ax_i+b) = b+\max_{1\leq i\leq n}(ax_i) = b+a\max_{1\leq i\leq n} x_i =a\max(x)+b.
\]
Por consiguiente, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, se tiene que
\[
y_i^* = \frac{y_i-\min(y)}{\max(y)-\min(y)} = \frac{\left(ax_i+b\right)-\left(a\min(x)+b\right)}{\left(a\max(x)+b\right)-\left(a\min(x)+b\right)} = \frac{a\left(x_i-\min(x)\right)}{a\left(\max(x)-\min(x)\right)} = x_i^*,
\]
que es lo deseado.
\end{soln}

\newpage
% Problema 9
\begin{soln}
(a) Como el soporte de $X$ es $[x_m, \infty)$, con $x_m>0$, la transformación $Y=\log(X)$ está bien definida, y $Y$ tiene soporte en $[\log(x_m), \infty)$. Además, la función $g(x)=\log(x)$ definida en $\RR^{+}$ es uno a uno y tiene inversa $g^{-1}(y)=e^y$, la cual es una función derivable, con $\frac{d}{dy}{g^{-1}}(y)=e^y$. Por lo tanto, como se cumple la relación $Y=\log(X)$ y por consiguiente $X=e^Y$, por el Teorema de Cambio de Variables $Y$ tiene densidad dada por
\[
f_Y(y) = \left\lvert \frac{dx}{dy} \right\rvert f_X(e^y) \mathds{1}_{[\log(x_m), \infty)}(y) = e^y \frac{\alpha x_m^{\alpha}}{e^{y(\alpha+1)}}\mathds{1}_{[\log(x_m), \infty)}(y) = \alpha {\left(\frac{x_m}{e^y}\right)}^\alpha \mathds{1}_{[\log(x_m), \infty)}(y).
\]
Notemos que esta última expresión puede ser escrita como
\[
f_Y(y) = \alpha e^{-\alpha \left(y-\log(x_m)\right)}\mathds{1}_{[0, \infty)}(y-\log(x_m)),
\]
de donde se puede observar que $Y \stackrel{d}{=} \log(x_m)+Exp(\alpha)$, en donde $Exp(\alpha)$ es una variable aleatoria con distribución exponencial de media $\frac{1}{\alpha}$. En particular, de aquí se sigue que la función de distribución acumulada de $Y$ es
\[
F_Y(y) = \begin{cases}
  0, & \text{si } y<\log(x_m),\\
  1-e^{-\alpha(y-\log(x_m))}, & \text{si } y\geq \log(x_m).
\end{cases}
\]

(b) Primero veamos que, dado $x>x_m$,
\[
\probability{X\geq x} = \int_{x}^{\infty} \frac{\alpha x_m^{\alpha}}{t^{\alpha+1}} dt = x_m^{\alpha} {\left[-t^{-\alpha}\right]}_{t=x}^{\infty} = {\left(\frac{x_m}{x}\right)}^{\alpha}.
\]
De este modo, la cola de $X$ decae de forma polinomial, del orden $x^{-\alpha}$. Por otro lado, si $y>\log(x_m)$,
\[
\probability{Y\geq y} = e^{\alpha \log(x_m)}e^{-\alpha y} = x_m^{\alpha} e^{-\alpha y},
\]
de donde podemos ver que la cola de $Y$ decae de forma polinomial, del orden $e^{-\alpha y}$ (más rápidamente que el decaimiento polinomial). Es decir, $X$ tiene colas más pesadas, y al transformarse a $Y$, cambia a colas más ligeras.

(c) Notemos que, como $Y=\log(X)$, para todo $y\in\RR$ se cumple que
\[
\probability{Y>y} = \probability{X>e^y},
\]
de modo que, como $e^y$ crece más rápido que $y$, las colas de $Y$ decaen más rápidamente de las de $X$, como lo visto con la distribución Pareto, en donde un decaimiento polinomial se convierte en uno exponencial. Además, como la función logaritmo es creciente y $\log(x)\leq \log(x+1)\leq x$ para todo $x>0$, por lo general $Y$ tiene un soporte más grande que $X$.

Más aún, por las propiedades de la función logarítmica, los cambios grandes en $X$ se reflejan en cambios más chicos de $Y$. Por ejemplo, si un valor de $X$ se duplica, en la transformación logarítmica el valor de $Y$ solo incrementa en $\log 2$ (cambios multiplicativos se transforman en cambios aditivos). Por consiguiente, si $X$ tiene colas muy pesadas, $Y$ tiende a distribuir el peso a lo largo de los reales y no tan concentrado en las colas; es decir, se ``acortan'' las colas largas. Además esto produce, por lo general, distribuciones más cercanas a la simetría, en especial cuando hay errores multiplicativos, que se convierten en errores aditivos al aplicar logaritmo, y el Teorema del Límite Central explica dicha simetría.
\end{soln}

\newpage
% Problema 10
\begin{soln}

\end{soln}

\newpage
% Problema 11
\begin{soln}
(a) Sea $x>0$, y veamos que
\[
\lim_{\lambda\to 0} (x^{\lambda}-1) = 1-1=0,
\]
mientras que $\lim_{\lambda\to 0} \lambda = 0$. Además, la función $\lambda \mapsto \lambda$ es derivable, con derivada igual a $1(\neq 0)$. Por lo tanto, ya que el siguiente límite existe, por la Regla de l'Hôpital se tiene que
\[
\log(x) = \lim_{\lambda\to 0} \frac{\log(x) x^\lambda}{1} = \lim_{\lambda\to 0} \frac{x^{\lambda}-1}{\lambda}=\lim_{\lambda\to 0} y(\lambda).
\]

(b) Consideremos a la sucesión ${(x_n)}_{n\in\NN}$, en donde $x_n = 2^n$ para todo $n\in\NN$. Dicha sucesión toma valores muy dispersos cuando $n$ es muy grande, pues sus primeros valores son
\[
2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, 4096, 8192, 16384, 32768, 65536, 131072, 262144, \ldots.
\]
La sucesión correpondiente a la transformación de Box y Cox con $\lambda=1$ es la misma pero recorrida en $1$, así que sigue siendo igual de dispersa. Sin embargo, con la transformación logarítmica ($\lambda=0$), se convierte en ${(y_n)}_{n\in\NN}$, en donde $y_n = n\log(2)$ para todo $n\in\NN$, que es mucho menos dispersa. A manera de ilustración, sus primeros valores son aproximadamente iguales a:
\[
0.6931, 1.3863, 2.0794, 2.7726, 3.4657, 4.1589, 4.852, 5.5452, 6.2383, 6.9315, 7.6246, 8.3178, 9.0109, \ldots
\]
\end{soln}

\newpage
% Problema 12
\begin{soln}
a)- Con las hipótesis del enunciado, observe que la función $\hat{f}_h(x)$ es una suma de funciones indicadoras, que cuenta el número de observaciones $x_i$ que están en el mismo conjunto que $x,$ $I_j.$ Luego ya que $1\{x_i \in I_j\} \geq 0$ y $nh>0$ se tiene que $\hat{f}_h(x)\geq 0.$
    \\ \\ 
    b)- Para dar respuesta a este inciso, observe que la función $\hat{f}_h(x)$ se puede escribir como,
$$ 
\hat{f}_h(x)=\frac{1}{n h} \sum_{i=1}^n 1\left\{x_i \in I_j\right\} 1\{x \in I_j\}.
$$
    Luego la integral buscada se puede ver como,
    $$\begin{aligned} & \int_{-\infty}^{\infty} \hat{f}_h(x) d x=\int_{-\infty}^{\infty} \frac{1}{n h} \sum_{i=1}^n 1\left\{x_i \in I_j\right\} 1\{x \in I_j\} d x \\ = & \sum_{j=1}^k \frac{1}{n h} \sum_{i=1}^n 1\left\{x_i \in I_j\right\} \int_{-\infty}^{\infty} 1\left\{d x \in I_j\right\} d x \\ = & \sum_{j=1}^n \frac{1}{n h} \sum_{i=1}^n 1\left\{x_i \in I_j\right\} \int_{I_j} d x \\ = & \sum_{j=1}^n \frac{1}{n h} \sum_{i=1}^n 1\left\{x_i \in I_j\right\} \cdot h \\ = & \frac{1}{n} \sum_{i=1}^n \sum_{j=1}^n 1\left\{x_i \in I_j\right\} \\ = & \frac{1}{n} \sum_{i=1}^n 1 \\ = & \frac{n}{n}=1.\end{aligned}$$

Donde la segunda igualdad se tiene de considerar la partición $I_1,...,I_k,$ la penúltima igualdad se tiene ya que cada $x_i$ pertenece a algún conjunto $I_j.$
\\ \\
c)-Observe que cuando h es grande, los intervalos contendrán más datos, esto nos llevará a que no se aprecie si hay algún patrón en el comportamiento de los datos, es decir si los datos tienen preferencia por ciertos intervalos. Por otro lado, cuando h es muy pequeño, los intervalos no alcanzarán a contener muchos datos, un caso extremo de ver esto es hacer a h muy cercano a cero de tal forma que cada intervalo contenga a lo más un dato, en este caso, solo se verán barras de tamaño $\frac{1}{n}$ en cada dato.

\end{soln}

\newpage
% Problema 13
\begin{soln}
 Normalización)- Con las hipótesis del enunciado, observe que la integral se puede escribir como, 
    $$\begin{aligned} & \int_{-\infty}^{\infty} \hat{f}_h(x) d x=\int_{-\infty}^{\infty} \frac{1}{nh} \sum_{i=1}^n K\left(\frac{x-x_i}{h}\right) d x \\ = & \sum_{i=1}^n \frac{1}{n h} \int_{-\infty}^{\infty} K\left(\frac{x-x_i}{h}\right) d x \\ = & \sum_{i=1}^n \frac{1}{n h} \int_{-\infty}^{\infty} K(u) h d u \\ = & \sum_{i=1}^n \frac{1}{n} \int_{-\infty}^{\infty} K(u) d u \\ = & \sum_{i=1}^n \frac{1}{n}=1.\end{aligned}$$
    Donde la tercer igualdad se tiene haciendo el cambio de variable $u=\frac{x-x_i}{h}$ y la penúltima igualdad se tiene gracias a que la integral del kernel K es uno.
    \\ \\
    No negatividad)- Para este inciso basta observar por hipotesis $K(u) \geq 0$ y que $nh>0$ por lo que $\hat{f}_h(x)\geq 0.$
    \\ \\ 
    Sesgo puntual)- Observe que el sesgo puntual se puede escribir como, 
$$\begin{aligned} & \mathbb{E}\left[\hat{f}_h(x)\right]-f(x)=\mathbb{E}\left[\sum_{i=1}^n \frac{1}{n h} K\left(\frac{x-x_i}{h}\right)\right]-f(x) \\ = & \sum_{i=1}^n \frac{1}{n h} \mathbb{E}\left[K\left(\frac{x-x_i}{h}\right)\right]-f(x) \\ = & \frac{1}{h} \mathbb{E}\left[K\left(\frac{x-x_1}{h}\right)\right]-f(x) \\ = & \frac{1}{h} \int_{-\infty}^{\infty} K\left(\frac{x-x_1}{h}\right) f\left(x_1\right) d x_1-f(x).\end{aligned}$$
Haciendo el cambio de variable $u=\frac{x-x_1}{h}$ y considerando que la integral del kernel es igual a uno, se tiene que la expresión anterior es igual a, 
$$\begin{aligned} & \frac{1}{h} \int_{-\infty}^{\infty} h K(u) f(x-u h) d u-\int_{-\infty}^{\infty} K(u) f(x) d u \\ = & \int_{-\infty}^{\infty} K(u)[f(x-u h)-f(x)] d u.\end{aligned}$$
Por otro lado, observe que si se desarrolla la serie de Taylor de orden dos de $f(x-uh)$ al rededor de $x$ con error de Peano se tiene la siguiente expresión, 
$$f(x-u h)=f(x)+f^{\prime}(x)(x-u h-x)+\frac{f^{\prime \prime}(x)(x-v h-x)^2}{2!}+h_2(x-uh)(uh)^2, \quad \lim_{x-uh \to x}h_2(x-uh)=0,$$
de donde se puede despejar la expresión $f(x-u h)-f(x)$ y sustituyendo en la integral se tiene, 
$$\begin{aligned} & \int_{-\infty}^{\infty} K(u)\left[f^{\prime}(x)(-u h)+\frac{f^{\prime \prime}(x)(u h)^2}{2!}+h_2(x-uh)(uh)^2\right] d u \\ = & \frac{f^{\prime \prime}(x)}{2!} h^2 \int_{-\infty}^{\infty} u^2 K(u) d u+h^2 \int_{-\infty}^{\infty} K(u) h_2(x-uh) u^2  d u,\end{aligned}$$
donde esta última igualdad se tiene ya que el primer momento es cero. Además, observe que si se divide entre $h^2$ a la expresión de la derecha y se hace tender a $h$ a cero se tiene,
\end{soln}

\end{document}