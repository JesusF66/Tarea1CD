\documentclass[twoside,12pt]{article}

\usepackage{geometry}
 \geometry{
 a4paper,
 left=20mm,
 right=20mm,
 }

\usepackage[spanish,activeacute,es-noindentfirst]{babel} %paqueteria para idioma
\spanishdecimal{.}
\usepackage{pstricks}
\usepackage[utf8]{inputenc}
%\usepackage{pstricks-add}


%Para Figuras en Tikz
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}
\usepackage{mathrsfs}

\usepackage{amssymb,amsfonts,amsbsy,amsmath}
\usepackage{graphicx}
\usepackage{mathtools}
\usepackage{amsthm}
\usepackage{dsfont}
\usepackage{float}
% \usepackage{enumitem}
\usepackage{enumerate} %Para poner \begin{enumerate}[a)]
\usepackage[hidelinks]{hyperref}

\usepackage{subfig}

\usepackage{rotating} %para rotar texto en tablas con \begin{sideways}
%\end{sideways}
\usepackage{makecell} %Para dividir celdas de tablas en varias l\'ineas


\theoremstyle{definition}
\newtheorem{probn}{Problema}
\newtheorem{soln}{Solución del problema}

\title{\textsc{Tarea 1\\ {\Large Ciencia de Datos}}}
\author{\large{\textbf{Brain de Jesús Salazar, César Ávila, Iván García}}}
\date{12 de septiembre de 2025}

\usepackage{fancyhdr}
\pagestyle{fancy}
\fancyhf{}
\renewcommand{\headrulewidth}{0.1pt}
\renewcommand{\footrulewidth}{0.1pt}
\fancyhead[LE]{\footnotesize{\textsc{Tarea 1}}}
\fancyhead[RO]{\footnotesize{\textsc{Ciencia de Datos}}}
\rfoot{\textsc{\thepage}}
\lfoot{\footnotesize{\textsc{Brain de Jesús Salazar, César Ávila, Iván García}}}

\usepackage{listings}

\def\sin{\mathop{\mathrm{\normalfont sen}}\nolimits}
\def\cos{\mathop{\mathrm{\normalfont cos}}\nolimits}

\newcommand{\RR}{\mathbb{R}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\PP}{\mathbb{P}}
\newcommand{\EE}{\mathbb{E}}
\newcommand{\XX}{\boldsymbol{X}}
\newcommand{\YY}{\boldsymbol{Y}}
\newcommand{\ZZ}{\boldsymbol{Z}}
\newcommand{\variance}{\mathrm{\normalfont Var}}
\newcommand{\covariance}{\mathrm{\normalfont Cov}}
\newcommand{\probability}[1]{\mathbb{P}\left[#1\right]}
\newcommand{\expectation}[1]{\mathbb{E}\left[#1\right]}
\newcommand{\given}{\,\middle|\,}

\begin{document}
\maketitle

% Problema 1
\begin{soln}

\end{soln}

\newpage
% Problema 2
\begin{soln}

\end{soln}

\newpage
% Problema 3
\begin{soln}

\end{soln}

\newpage
% Problema 4
\begin{soln}

\end{soln}

\newpage
% Problema 5
\begin{soln}
Consideremos una muestra $Y_1, \ldots, Y_n$ de variables aleatorias independientes e idénticamente distribuidas con media $\mu$ y varianza $\sigma^2$, con indicadores $R_i \in \left\lbrace 0, 1 \right\rbrace$, de tal manera que $R_i\perp Y_i$ para cada $i\in \left\lbrace 1, \ldots, n \right\rbrace$. Dichas $R_i$ existen pues estamos bajo el modelo MCAR, y se interpretan como $R_i = 1$ si y solo si el dato $Y_i$ fue observado.

Notemos que si $n_{obs}$ representa el número de datos observados, entonces $n_{obs} = \sum_{i=1}^{n} R_i$. Además, por la definición de los $R_i$,
\[
\overline{Y}_{obs} = \frac{1}{n_{obs}} \sum_{i=1}^{n} R_i Y_i = \frac{1}{n_{obs}} \sum_{i: R_i=1} Y_i.
\]
Así pues, si $\boldsymbol{R}=(R_1, \ldots, R_n)$, entonces
\[
\begin{split}
  \expectation{\overline{Y}_{obs} \given \boldsymbol{R}} = \expectation{\frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i Y_i  \given \boldsymbol{R}} &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} \expectation{R_i Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i \given \boldsymbol{R}}\\
  &= \frac{1}{\sum_{i=1}^{n} R_i} \sum_{i=1}^{n} R_i\expectation{Y_i}\\
  &=\mu,
\end{split}
\]
en donde hemos usado que las $Y_i$ son iid con media $\mu$ y son independientes de $\boldsymbol{R}$ (por las hipótesis del modelo MCAR). Por consiguiente,
\[
\expectation{\overline{Y}_{obs}} = \expectation{\expectation{\overline{Y}_{obs} \given \boldsymbol{R}}} = \mu.
\]
Por otra parte,
\[
\overline{Y}_{obs}^2 = \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n}R_i^2 Y_i^2 + \sum_{i\neq j}R_i R_j Y_i Y_j\right],
\]
por lo que
\[
\begin{split}
  \expectation{\overline{Y}_{obs}^2 \given \boldsymbol{R}} &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2 \given \boldsymbol{R}} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j \given \boldsymbol{R}}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[\sum_{i=1}^{n} R_i^2 \expectation{Y_i^2} + \sum_{i\neq j}R_i R_j \expectation{Y_i Y_j}\right]\\
  &= \frac{1}{{\left(\sum_{i=1}^{n} R_i\right)}^2} \left[(\sigma^2+\mu^2)\sum_{i=1}^{n} R_i^2 + \mu^2\sum_{i\neq j}R_i R_j\right]\\
  &= \mu^2+\sigma^2\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}.
\end{split}
\]
De lo anterior se sigue que
\[
\begin{split}
  \variance \left[\overline{Y}_{obs}\right] = \expectation{\overline{Y}_{obs}^2}-{\left(\expectation{\overline{Y}_{obs}}\right)}^2 &= \expectation{\expectation{\overline{Y}_{obs}^2\given \boldsymbol{R}}}-\mu^2
  = \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i^2}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{\sum_{i=1}^{n} R_i}{{\left(\sum_{i=1}^{n} R_i\right)}^2}}\\
  &= \sigma^2 \expectation{\frac{1}{\sum_{i=1}^{n} R_i}}\\
  &= \sigma^2 \expectation{\frac{1}{n_{obs}}}\\
  &\geq \frac{\sigma^2}{n} = \variance \left[\ \overline{Y}\ \right],
\end{split}
\]
en donde hemos usado que $R_i^2 = R_i$, pues $R_i\in \left\lbrace 0, 1 \right\rbrace$, para todo $i\in \left\lbrace 1, \ldots, n \right\rbrace$, y que $n_{obs}\leq n$. De hecho, siguiendo un procedimiento completamente análogo, pero ahora con varianzas condicionales, se sigue que
\[
\variance \left[\overline{Y}_{obs} \given \boldsymbol{R}\right] = \frac{\sigma^2}{n_{obs}}.
\]
De lo anterior podemos notar que $\overline{Y}_{obs}$ es insesgado, pero que $\variance \left[\overline{Y}_{obs}\right] \geq \variance \left[\ \overline{Y}\ \right]$, por lo que $\overline{Y}_{obs}$ tiene menor eficiencia (posee más varianza, pues la eliminación de datos hace que haya menos de ellos para poder estimar a la media de $Y$).
\end{soln}

\newpage
% Problema 6
\begin{soln}
Sean $\YY = (Y_{obs}, Y_{mis})$ y $\boldsymbol{R}$ el patrón de datos faltantes. Bajo la definición del MAR,
\[
\probability{\boldsymbol{R} \given Y_{obs}, Y_{mis}, \theta, \psi} = \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Así pues, bajo este modelo,
\[
\probability{\YY, \boldsymbol{R} \given \theta, \psi} = \probability{\YY \given \theta} \probability{\boldsymbol{R} \given \YY, \psi} =  \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi}.
\]
Por consiguiente, la verosimilitud de $\theta$ está dada por
\[
\begin{split}
  L\left(\theta; Y_{obs}, \boldsymbol{R}\right) = \int \probability{\YY, \boldsymbol{R} \given \theta, \psi} dY_{mis} &= \int \probability{\YY \given \theta} \probability{\boldsymbol{R} \given Y_{obs}, \psi} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \int \probability{\YY \given \theta} dY_{mis}\\
  &= \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta}.
\end{split}
\]
Ya que el factor $\probability{\boldsymbol{R} \given Y_{obs}, \psi}$ no depende de $\theta$, se sigue que $L\left(\theta; Y_{obs}, \boldsymbol{R}\right) \propto \probability{Y_{obs} \given \theta}$. Para ver las condiciones \textit{a priori} que garantizan ignorabilidad bajo el enfoque bayesiano, notemos que
\[
\begin{split}
  \probability{\theta \given Y_{obs}, \boldsymbol{R}} = \int \probability{\theta, \psi \given Y_{obs}, \boldsymbol{R}} d\psi &\propto \int \probability{Y_{obs}, \boldsymbol{R} \given \theta, \psi} \pi\left(\theta, \psi\right) d\psi\\
  &= \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \probability{Y_{obs} \given \theta} \pi\left(\theta, \psi\right) d\psi\\
  &= \probability{Y_{obs} \given \theta} \int \probability{\boldsymbol{R} \given Y_{obs}, \psi} \pi\left(\theta, \psi\right) d\psi.
\end{split}
\]
Para concluir ignorabilidad buscamos que $L\left(\theta \given Y_{obs}, \boldsymbol{R}\right) \propto \pi(\theta) \probability{Y_{obs} \given \theta}$, y ya que la integral anterior depende de $\theta$ solamente a través del factor $\pi\left(\theta, \psi\right)$, dicha ignorabilidad se logra cuando $\pi(\theta, \psi)=\pi(\theta)\pi(\psi)$. Es decir, si en la \textit{a priori} se pide indistinguibilidad de los parámetros (i.e. que $\theta$ y $\psi$ sean independientes), entonces el mecanismo es ignorable para inferir $\theta$.
\end{soln}

\newpage
% Problema 7
\begin{soln}

\end{soln}

\newpage
% Problema 8
\begin{soln}

\end{soln}

\newpage
% Problema 9
\begin{soln}

\end{soln}

\newpage
% Problema 10
\begin{soln}

\end{soln}

\newpage
% Problema 11
\begin{soln}

\end{soln}

\newpage
% Problema 12
\begin{soln}

\end{soln}

\newpage
% Problema 13
\begin{soln}

\end{soln}

\end{document}